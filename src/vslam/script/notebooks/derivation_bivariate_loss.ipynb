{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to solve:\n",
    "\n",
    "$$ \\sum_i \\frac{\\partial \\log p(r_i | \\theta )}{\\partial \\theta} = \\sum_i \\frac{\\partial \\log p(r_i)}{\\partial r_i} \\frac{\\partial r_i}{\\partial \\theta} = 0 $$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the weighting function as:\n",
    "\n",
    "$$ w_i = \\frac{\\partial \\log p(r_i)}{\\partial r_i} \\frac{1}{r_i}$$\n",
    "\n",
    "and get:\n",
    "\n",
    "$$ \\sum_i w_i(r_i) r_i \\frac{\\partial r_i}{\\partial \\theta}  = 0$$\n",
    "\n",
    "Which minimizes the weighted least squares problem:\n",
    "\n",
    "$$ \\argmin_{\\theta} \\sum_i w_i(r_i) r_i(\\theta)^2 $$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume the residuals to be gaussian distributed around: $p(r_i) = \\exp(\\frac{r^2}{\\sigma^2})$\n",
    "\n",
    "The weight becomes:\n",
    "\n",
    "$$ w_i(r_i) = 2 \\frac{r}{\\sigma^2} \\frac{1}{r} = \\frac{2}{\\sigma^2} $$\n",
    "\n",
    "and thus:\n",
    "\n",
    "$$ \\sum_i \\frac{2}{\\sigma^2} r_i \\frac{\\partial r_i}{\\partial \\theta}  = 0$$\n",
    "\n",
    "$$ \\sum_i \\frac{r_i}{\\sigma^2} \\frac{\\partial r_i}{\\partial \\theta}  = 0$$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically $r(\\theta)$ is non linear and instead of beeing able to directly solve for the optimal parameters we have to iteratively linearize $r(\\theta)$, solve for a parameter update $\\Delta \\theta$ around the linearization point and update the parameters until convergence.\n",
    "\n",
    "The linearization by first order taylor expansion typically look like this:\n",
    "\n",
    "$$ r \\approx r(\\theta_0) + J \\Delta \\theta$$\n",
    "\n",
    "Where $J$ is the jacobian of $r$ with respect to the parameters $\\theta$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging in the linearized loss we get:\n",
    "\n",
    "$$\\sum_i w_i (r_i + J_i\\Delta \\theta)J_i$$\n",
    "\n",
    "$$\\sum_i (w_i r_iJ + w_iJ\\Delta \\theta J_i)$$\n",
    "\n",
    "For the gaussian case:\n",
    "\n",
    "$$\\sum_i \\frac{1}{\\sigma^2}(r_iJ + J\\Delta \\theta J_i)$$\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially we assumed the samples to be i.i.d and used the logarithm to convert the product of all observations to a sum. If we want to include additional constraints per observation we can use a combined probability as in $r^T\\Sigma^{-1}T$, where $r$ is a vector with different residuals per observation $i$. Then the least squares problem becomes:\n",
    "\n",
    "$$ \\sum_i w_i(r^T\\Sigma^{-1}r) r^T\\Sigma^{-1}r $$\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plugging in the two linearized residuals ($r_I, r_Z$) as before we get:\n",
    "\n",
    "$$ \\sum_i w_i(r^T\\Sigma^{-1}r) \\begin{bmatrix} \n",
    "\tr_I + J_I\\Delta \\theta & r_Z + J_Z\\Delta \\theta\n",
    "\t\\end{bmatrix} \\Sigma^{-1} \\begin{bmatrix} \n",
    "\tr_I + J_I\\Delta \\theta \\\\\n",
    "    r_Z + J_Z\\Delta \\theta\n",
    "\t\\end{bmatrix}$$\n",
    "\n",
    "$$ \\sum_i w_i \\begin{bmatrix}  \n",
    "\tr_I + J_I\\Delta \\theta & r_Z + J_Z\\Delta \\theta\n",
    "\t\\end{bmatrix} \\begin{bmatrix} \\Sigma^{-1}_{11} (r_I + J_I) + \\Sigma^{-1}_{12} (r_Z + J_Z) \\\\\n",
    "\\Sigma^{-1}_{21} (r_I + J_I) + \\Sigma^{-1}_{22} (r_Z + J_Z)\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "$$\n",
    "\\sum_i w_i (r_I + J_I)(\\Sigma^{-1}_{11} (r_I + J_I) + \\Sigma^{-1}_{12} (r_Z + J_Z)) + (r_Z + J_Z)(\\Sigma^{-1}_{21} (r_I + J_I) + \\Sigma^{-1}_{22} (r_Z + J_Z))\n",
    "$$\n",
    "\n",
    "We see we have each combination multiplied and normalized by their (co)variance, which is simply the expression for the univariate case but multiplied by the (co)variance:\n",
    "\n",
    "$$ (r + J\\Delta \\theta) \\Sigma^{-1}_* (r + J\\Delta \\theta)$$\n",
    "\n",
    "If we derive and set to 0 we get a similar result as before:\n",
    "\n",
    "$$ \\Sigma^{-1}_(Jr + J^TJ\\Delta \\theta) = 0$$\n",
    "\n",
    "Plugging all combinations in a matrix formulation yields:\n",
    "\n",
    "$$ w_i(r_i^T\\Sigma^{-1}r_i) J_i\\Sigma^{-1}r_i + J_i^T\\Sigma^{-1}J\\Delta \\theta = 0\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
